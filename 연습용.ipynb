{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", message=\"stft with return_complex=False is deprecated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(debug=True, gpu_index='0', num_workers=8, verbose=1, save_path=None, train_datapath=PosixPath('/Data2/murmur/train'), test_datapath=PosixPath('/Data2/murmur/test'), external_datapath=PosixPath('/Data2/heart_sound_dataset'), external_data_subpath={'pysionet_sufhsdb': 'pysionet_sufhsdb', 'kaggle_set_a': 'kag_dataset_1/set_a', 'kaggle_set_b': 'kag_dataset_1/set_b'}, num_k=5, val_fold_num=4, sampling_rate=4000, window_length=0.05, hop_length=0.02, freq_high=800, freq_bins=40, train_seq_len=6, clean_noise=True, random_seed=0, max_epoch=250, train_bs=80, unlabel_bs=160, val_bs=120, learning_rate=0.0005, base_lr=4e-05, max_lr=0.001, final_lr=1e-05, training_patience=10, pos_weight=4.0, ema_factor=0.99, const_max=1, use_mixup=False, mixup_alpha=0.2, mixup_beta=0.2, mixup_label_type='soft')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = get_args()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = config.train_datapath\n",
    "external_datapath = config.external_datapath\n",
    "external_data_subpath = config.external_data_subpath\n",
    "test_data_folder = config.test_datapath\n",
    "\n",
    "num_k = config.num_k\n",
    "val_fold_num = config.val_fold_num\n",
    "\n",
    "split_ratio= 0.1 if config.debug else 1   \n",
    "split_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = load_patient_files(data_folder, stop_frac= split_ratio)\n",
    "patient_df = create_folds(patient_df, num_k)\n",
    "recording_df = patient_df_to_recording_df(patient_df)\n",
    "\n",
    "recording_df_gq = recording_df[recording_df[\"patient_murmur_label\"] != \"Unknown\"]\n",
    "recording_df_bq = recording_df[recording_df[\"patient_murmur_label\"] == \"Unknown\"]\n",
    "\n",
    "train_recording_df = recording_df_gq[recording_df_gq['val_fold'] != val_fold_num]\n",
    "val_recording_df = recording_df_gq[recording_df_gq[\"val_fold\"] == val_fold_num]   \n",
    "\n",
    "weaklabeled_df, unlabeled_df = get_external_df(external_datapath, config.external_data_subpath)\n",
    "concat_unlabeld_df = pd.concat([weaklabeled_df, unlabeled_df], axis= 0).reset_index(drop=True)\n",
    "\n",
    "test_patient_df = load_patient_files(test_data_folder, stop_frac= split_ratio)\n",
    "test_recording_df = test_patient_df_to_recording_df(test_patient_df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  data_folder, recording_paths, timings, murmur_labels, outcome_labels, \n",
    "#                  sampling_rate, window_length, window_step,\n",
    "#                  clean_noise\n",
    "\n",
    "train_dataset =  Stronglabeled_Dataset(data_folder, \n",
    "                            train_recording_df.index, train_recording_df.murmur_timing, \n",
    "                            train_recording_df.patient_murmur_label, train_recording_df.outcome_label,\n",
    "                            sampling_rate= config.sampling_rate,\n",
    "                            window_length= config.window_length,\n",
    "                            window_step= config.hop_length,\n",
    "                            clean_noise= config.clean_noise\n",
    "                            )\n",
    "unlabeled_dataset = Unlabeled_Dataset(external_datapath, \n",
    "                                    concat_unlabeld_df.mid_path, concat_unlabeld_df.filename, \n",
    "                                    config.sampling_rate, config.window_length, config.hop_length, config.freq_bins)\n",
    "\n",
    "val_dataset = Stronglabeled_Dataset(data_folder, \n",
    "                            val_recording_df.index, val_recording_df.murmur_timing, \n",
    "                            val_recording_df.patient_murmur_label, val_recording_df.outcome_label,\n",
    "                            sampling_rate= config.sampling_rate,\n",
    "                            window_length= config.window_length,\n",
    "                            window_step= config.hop_length,\n",
    "                            clean_noise= config.clean_noise\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1922, 626, 481)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(unlabeled_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "sampler_unlabeled = RandomSampler(unlabeled_dataset, replacement=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size= config.train_bs, shuffle=True, collate_fn= collate_fn, num_workers=config.num_workers)\n",
    "unlabeled_dataloader = DataLoader(unlabeled_dataset, batch_size= config.unlabel_bs, sampler= sampler_unlabeled, collate_fn= collate_fn, num_workers=config.num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size= config.val_bs, shuffle=False,  collate_fn= collate_fn, num_workers=config.num_workers) \n",
    "\n",
    "for i, (batch_strong, batch_unlabel) in enumerate(zip(train_dataloader, cycle(unlabeled_dataloader))):\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(x):\n",
    "    # max_length = cal_max_frame_len(config.sampling_rate, config.sequence_length)\n",
    "    # max_length= MAX_LENGTH\n",
    "    max_length = 298\n",
    "    FREQ_BINS = 40\n",
    "    \n",
    "    all_features = torch.zeros(len(x), FREQ_BINS, max_length)  # [bs, freq, frame]\n",
    "    all_seq_labels = torch.ones(len(x), 3, max_length) * (-1) # S1, S2, Murmur, \n",
    "    all_murmur_labels = []\n",
    "    all_outcome_labels = []\n",
    "    pad_masks = []    \n",
    "    all_filenames = []\n",
    "    \n",
    "    for idx, (features, seq_label, murmur_label, outcome_label, wav_len, filename) in enumerate(x):\n",
    "        \n",
    "        pad_mask = torch.ones(max_length)\n",
    "        \n",
    "        # 같거나 짧음\n",
    "        if features.shape[-1] <= max_length:\n",
    "            diff = max_length - features.shape[-1]\n",
    "            start = random.randint(0, diff)\n",
    "            end = start + features.shape[-1]\n",
    "            all_features[idx, :, start : end] = features\n",
    "            all_seq_labels[idx, : , start : end] = seq_label\n",
    "            \n",
    "            pad_mask[start:end] = 0.0 # 데이터 있는 부분이 0.0\n",
    "            pad_masks.append(pad_mask)\n",
    "            # actual_seq_lengths.append(len(seq_label))\n",
    "            all_filenames.append(filename)\n",
    "        # 더 길면\n",
    "        else:\n",
    "            diff = features.shape[-1] - max_length\n",
    "            start = random.randint(0, diff)\n",
    "            end =  start + max_length\n",
    "            all_features[idx, :, :] = features[:, start : end]\n",
    "            all_seq_labels[idx, :, :] = seq_label[:, start : end]\n",
    "            \n",
    "            pad_mask[:] = 0.0 # 모든 시퀀스에 데이터 있으므로 모두 0.0\n",
    "            pad_masks.append(pad_mask)\n",
    "            # actual_seq_lengths.append(max_length)\n",
    "\n",
    "        all_murmur_labels.append(murmur_label)\n",
    "        all_outcome_labels.append(outcome_label)\n",
    "        all_filenames.append(filename)\n",
    "        \n",
    "    all_features = all_features.float()\n",
    "    all_seq_labels = all_seq_labels.float()\n",
    "    all_murmur_labels = torch.stack(all_murmur_labels).float()\n",
    "    pad_masks = torch.stack(pad_masks).bool() #.float()\n",
    "\n",
    "    return (all_features, all_seq_labels, pad_masks, all_murmur_labels, all_outcome_labels, all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MHA_LSTM_simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act(act_func_name):\n",
    "    if act_func_name.lower() == \"relu\":\n",
    "        act_func = nn.ReLU()\n",
    "    elif act_func_name.lower() == 'leakyrelu':\n",
    "        act_func = nn.LeakyReLU()\n",
    "    elif act_func_name.lower() == \"gelu\":\n",
    "        act_func = nn.GELU()\n",
    "    elif act_func_name.lower() == 'tanh':\n",
    "        act_func = nn.Tanh()\n",
    "    else:\n",
    "        raise ValueError(\"you should add activation func in 'get_act function'\")\n",
    "    return act_func\n",
    "\n",
    "\n",
    "# input_dim, hidden_dims, output_dim, act_func_name, dropout_rate, \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, act_func_name, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = nn.Sequential(nn.Linear(input_dim, hidden_dims[0]), \n",
    "                            nn.LayerNorm(hidden_dims[0]) , \n",
    "                            get_act(act_func_name),\n",
    "                            nn.Dropout(dropout_rate),\n",
    "                            )\n",
    "        \n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
    "            layers.append(get_act(act_func_name))\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA_LSTM_final(nn.Module):\n",
    "    def __init__(self, model_params) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(**model_params[\"rnn_params\"])\n",
    "           \n",
    "        self.selfattn_layer = nn.MultiheadAttention(**model_params[\"attn_module\"][\"MHA_params\"])\n",
    "        self.layer_norm1 = nn.LayerNorm(model_params[\"attn_module\"][\"layernorm_dim\"])\n",
    "        \n",
    "        self.frame_linear = MLP(**model_params[\"frame_linear\"])\n",
    "        \n",
    "        \n",
    "        self.murmur_linear = nn.Sequential(\n",
    "            nn.Linear(model_params[\"murmur_linear\"][\"input_dim\"], model_params[\"murmur_linear\"][\"output_dim\"]),\n",
    "            )    \n",
    "    \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.uniform_(m.weight, -0.1, 0.1)\n",
    "                if m.bias is not None:\n",
    "                    init.uniform_(m.bias, -0.1, 0.1)\n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                for param in m.parameters():\n",
    "                    if param.dim() >= 2:  # weight matrices\n",
    "                        init.uniform_(param, -0.1, 0.1)\n",
    "                    else:  # bias vectors\n",
    "                        init.uniform_(param, -0.1, 0.1)\n",
    "            elif isinstance(m, nn.MultiheadAttention):\n",
    "                init.uniform_(m.in_proj_weight, -0.1, 0.1)\n",
    "                if m.in_proj_bias is not None:\n",
    "                    init.uniform_(m.in_proj_bias, -0.1, 0.1)\n",
    "                init.uniform_(m.out_proj.weight, -0.1, 0.1)\n",
    "                if m.out_proj.bias is not None:\n",
    "                    init.uniform_(m.out_proj.bias, -0.1, 0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, pad_mask= None):        \n",
    "        x = x.permute(0, 2, 1) # [B, D, T] >> [B, T, D]\n",
    "        \n",
    "        # Bi-Rnn\n",
    "        rnn_out, h_n = self.rnn(x)\n",
    "        residual = rnn_out\n",
    "        \n",
    "        # Self MultiHeadAttention\n",
    "        attn_output, attn_weights = self.selfattn_layer(rnn_out, rnn_out, rnn_out, key_padding_mask= pad_mask)\n",
    "\n",
    "        attn_output += residual\n",
    "        attn_output = self.layer_norm1(attn_output)\n",
    "        \n",
    "        seq_pred = self.frame_linear(attn_output).permute(0, 2, 1) # [B, T, C] >> [B, C, T]\n",
    "\n",
    "        mm_linear_input = F.sigmoid(seq_pred).mean(dim=-1)[:, -1] # [B, C, T] >> [B, C] >> [B, Murmur]\n",
    "        murmur_pred = self.murmur_linear(mm_linear_input.unsqueeze(-1).detach())\n",
    "        return seq_pred, murmur_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {}\n",
    "\n",
    "model_params['rnn_params'] =  {\"input_size\": 40, \n",
    "                                \"hidden_size\": 60, \n",
    "                                \"num_layers\": 3, \n",
    "                                \"batch_first\": True, \n",
    "                                \"bidirectional\": True, \n",
    "                                \"dropout\": 0.1,\n",
    "                                             }\n",
    "                             \n",
    "model_params[\"attn_module\"] = {\"MHA_params\": {\"embed_dim\": model_params['rnn_params'][\"hidden_size\"] * 2, \n",
    "                                                    \"num_heads\": 4, \n",
    "                                                    \"batch_first\": True, },\n",
    "                               \n",
    "                               \"layernorm_dim\": model_params['rnn_params'][\"hidden_size\"] * 2}\n",
    "\n",
    "model_params[\"frame_linear\"] = {\"input_dim\": model_params['rnn_params'][\"hidden_size\"] * 2, \n",
    "                                \"hidden_dims\": [40],\n",
    "                                \"output_dim\": 3,\n",
    "                                \"act_func_name\": 'gelu', \n",
    "                                \"dropout_rate\": 0.3,\n",
    "                                }\n",
    "\n",
    "model_params[\"murmur_linear\"] = {\"input_dim\": 1, \n",
    "                                 \"output_dim\": 2}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MHA_LSTM_simpler(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA_LSTM_simpler(\n",
      "  (rnn): LSTM(40, 60, num_layers=3, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (selfattn_layer): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=120, out_features=120, bias=True)\n",
      "  )\n",
      "  (layer_norm1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
      "  (frame_linear): MLP(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=120, out_features=40, bias=True)\n",
      "      (1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "      (4): Linear(in_features=40, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (murmur_linear): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
